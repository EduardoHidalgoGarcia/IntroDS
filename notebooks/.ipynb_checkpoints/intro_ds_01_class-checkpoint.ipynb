{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Programming\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "# ***Datasets***\n",
    "\n",
    "<br/>\n",
    "\n",
    "1. [Setup](#0)<br>\n",
    "2. [About Pandas](#I)<br>\n",
    "3. [Creating & Reading](#II)<br>\n",
    "    3.1 [Creating data](#II.I)<br>\n",
    "    3.2 [Reading data](#II.II)<br>\n",
    "4. [Indexing, Selecting and Assigning](#III)<br>\n",
    "    4.1 [Naitive Python accessors](#III.I)<br>\n",
    "    4.2 [Indexing using Pandas syntaxis](#III.II)<br>\n",
    "    4.3 [Manipulating the index](#III.III)<br>\n",
    "    4.4 [Conditional Selection](#III.VI)<br>\n",
    "    4.5 [Assigning data](#III.V)<br>\n",
    "5. [Summary Functions and Maps](#VI)<br>\n",
    "    5.1 [Summary functions](#VI.I)<br>\n",
    "    5.2 [Maps](#VI.II)<br>\n",
    "6. [Grouping and Sorting](#V)<br>\n",
    "    6.1 [Grouping](#V.I)<br>\n",
    "    6.2 [Sorting](#V.II)<br>\n",
    "7. [Data Types and Missing Values](#IV)<br>\n",
    "    7.1 [Data Types](#IV.I)<br>\n",
    "    7.2 [Missing data](#IV.II)<br>\n",
    "8. [Renaming and Combining](#IIV)<br>\n",
    "    8.1 [Renaming](#IIV.I)<br>\n",
    "    8.2 [Combining](#IIV.II)<br>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Setup](#0)\n",
    "<a id=\"0\"></a>\n",
    "<br/>\n",
    "\n",
    "To see where is the *current working directory* for this specific `jupyter notebook` we leverage the method `getcwd()` from package `os`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Current directory* of a specific `jupyter notebook` is usually originally set to where the file is stored. Remember that during the first part of this course we learn that it is possible to change our *current working directory* by leveraging the method `chdir()` from the package `os`. \n",
    "\n",
    "The `while` loop above will *check* if the ending string returned by `os.getcwd()` is equal to \"IntroDS\", while this condition it will keep going one folder backwards.\n",
    "\n",
    "**IntroDS** is the name of the folder we have created for this second part of the course. The structure of it is as follows:\n",
    "\n",
    "+ data\n",
    "    + Data_Extract_From_World_Development_Indicators-OCDE-BRCH-EU.xlsx\n",
    "    + WorldBankDataReshaped.csv\n",
    "+ html\n",
    "    + NOVASBEIP2020-Class.html\n",
    "    + NOVASBEIP2020.html\n",
    "+ images\n",
    "    + DSKC_logo.png\n",
    "+ metadata\n",
    "    + Expense.xlsx\n",
    "    + Final Consumption Expenditure.xlsx\n",
    "    + Militar Expenditure.xlsx\n",
    "+ notebooks\n",
    "    + NOVASBEIP2020.html\n",
    "    + NOVASBEIP2020.ipynb\n",
    "    + ReshapingWorldBankData.R\n",
    "+ README.md\n",
    "    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [About pandas](#I)\n",
    "<a id=\"I\"></a>\n",
    "<br/>\n",
    "\n",
    "The most popular `Python` library for data analysis is `pandas`. In this part we will learn how to create our own data, along with how to work with data that already exists (i.e. how to import it to `Python`). To use `pandas` you will typically start with the following line of code.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(note that Anaconda environments known as conda already includes the `pandas`*package*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. [Creating & Reading data](#II)\n",
    "<a id=\"II\"></a>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 [Creating data](#II.I)\n",
    "<a id=\"II.I\"></a>\n",
    "<br/>\n",
    "\n",
    "There are two core objects in pandas: the `DataFrame` and the `Series`.\n",
    "<br/>\n",
    "\n",
    "#### 3.1.1 `DataFrame`\n",
    "\n",
    "<hr>\n",
    "\n",
    "A `DataFrame`is a table that contains and array of individual entries, each of which has a certain value. Each entry corresponds to a row (or record) and a column.\n",
    "\n",
    "As an example let's generate our first `DataFrame`. For this we will need to declare a `dictionary` and then feed it to the `pandas` method `DataFrame()`. \n",
    "\n",
    "`Dictionaries` are one of the basic `Python` data structures (as we learned on the first part of this course). If you are familiar with other programming languages you can think of them as mappings or collection of objects that are stored by a *key*, unlike other structures such as sequences or lists that store objects by their relative position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the entry indexed by (\"0\", \"Key 1\") corresponds to \"Value 1\". The (\"0\",\"Key 2\") value corresponds to \"Value 3\" and so on.\n",
    "\n",
    "`DataFrame` entries are not limited to `strings`. For instance, here is a `DataFrame` whose values are not strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the `DataFrame()` constructor from the library `pandas` that we declared as `pd` to generate these `DataFrame` objects. The syntax for declaring a new one is a dictionary whose keys are the column names (`Yes`and `No` in the prior example). This is the standard way of constructing a new DataFrame, and the one you are most likely to encounter.\n",
    "\n",
    "The dictionary-list constructor assigns values to the column labels, but just uses an ascending count from 0 (0, 1, 2, 3, ...) for the `row labels`. Sometimes this is OK, but oftentimes we will want to assign these labels ourselves.\n",
    "\n",
    "The list of `row labels` used in a `DataFrame` is known as an **Index**. We can assign values to it by using an `index` parameter in our constructor, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "##### 3.1.2 `Series`\n",
    "\n",
    "<hr>\n",
    "\n",
    "A `Series` is a sequence of data values. If a `DataFrame` is a **table**, a `Series` is a **list**, both with special methods and constructors than the ones available by using the `Matrix` (two-dimensional array) and `list` data structures. So, it is possible to generate a `pandas` `Series` with nothing more than a list, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Series` is, in essence, a single column of a `DataFrame`. So you can assign column values to the `Series` the same way as before, using an `index` parameter. However"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "**Summarizing**, we saw that `pandas` library has two main objects `DataFrame` and `Series`. By this time it must be clear that they are intimately related. It's helpful to think of a DataFrame as actually being just a bunch of Series \"glued together\"\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### 3.2 [Reading data](#II.II)\n",
    "<a id=\"II.II\"></a>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Data can be stored in any of a number of different forms and formats. By far the most basic format is the CSV file or *Comma-Separated Values*. The function from pandas that will allow us to read this format to Python is `read_csv()`. Now, lets read our example database using `read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `shape` **attribute** to check the dimensions of the resulting `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our `DataFrame` has 59 k records split accross 6 different columns. That is almost 356 k entries!\n",
    "\n",
    "We can examine the contents of the resultant `DataFrame` using the `head()` **method**, which grabs, by default, the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas `read_csv()` functions is well-endowed, with over 30 optional parameters for you to specify as needed. This optional parameters can allow you to read other types of format files. For example, by specifying the `sep='\\t'` this same function will allow you to read **tsv** files. Another optional parameter is the `index_col` which allows you to use the specified column (you will need to specify the number of the column starting by 0) as the `row_labels` of the `DataFrame`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very common format is the **xlsx** or Excel files. Unfortunately, `read_csv()` method from pandas library will not let us read this type of data. For reading into Python this format we need to install the package `xlrd`. Once we do that we can simply use the pandas method `read_excel()` in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset above is the extraction as downloaded from [World Bank's databank](https://databank.worldbank.org/data/source/world-development-indicators#). \n",
    "\n",
    "So far we have declared to our *environment* two <code>dataframe</code>  objects: `data` & `row_data`. The first one has been preprocessed in such a way to reshape it from a wide to a long format. The only difference is that for each country data belonging to a specific series was horizonatally presented (in raw data), but now it is in a *panel data* fashion. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> that you can always access to the <i>specifications</i> of a method by simply ending the method statement with a question mark instead of parenthesis. For example, lets see the additional parameters available in the <code>read_excel</code> method.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read_excel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, note that optional parameter `sheet_name` will let you pick the Excel worksheet of your choice (the default one is the first one indexed by number 0). \n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Exercise \n",
    "\n",
    "<hr>\n",
    "\n",
    "Use the information about the parameters of method `read_excel()` mentioned above to:\n",
    "+ Read the `Data_Extract_From_World_Development_Indicators-OCDE-BRCH-EU.xlsx` file using col `Country Code` as `row_labels`\n",
    "+ Use attribute `shape` to check the dimensions of it\n",
    "+ Use the method `head()` to let us visualize the 3 first rows of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 4. [Indexing, Selecting and Assigning](#III)\n",
    "<a id=\"III\"></a>\n",
    "<br/>\n",
    "\n",
    "Selecting specific values of a pandas `DataFrame` or `Series` to work on is an implicit step in almost any data operation you'll run, so one of the first things you need to learn in working with data in Python is how to go about selecting the data points relevant to you quickly and effectively.\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 [Naitive Python accessors](#III.I)\n",
    "<a id=\"III.I\"></a>\n",
    "<br/>\n",
    "\n",
    "Native Python objects provide good ways of indexing data. Pandas carries all of these over, which helps make it easy to start with.\n",
    "\n",
    "In Python, we can access the property of an object by accessing it as an attribute. A `book` object, for example, might have a `title` property, which we can access by calling `book.title`. **Columns in a pandas** `DataFrame` **work in much the same way**.\n",
    "\n",
    "Hence to access the `Continent` **property** of our `data` we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python dictionaries we can access its values by using the indexing `[ ]` operator. And, so we can do the same with columns in a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Or `[[ ]]` for a `list` of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas `Series` are pretty much like `list`s wrapped inside a fancy dictionary. And, so we can select a specific value by using the indexing operator once more, for example lets address the first observation fo the column `Country Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "<b>Summarizing:</b> As inherit forms from Python naitive objects we have <b>two</b> ways to select columns,  by <b>attribute</b> of the <code>DataFrame</code> or by <b>key</b> of the <code>dictionary</code> we can select specific <code>Series</code> out of a <code>DataFrame</code>. Neither of them is more or less syntactically valid than the other, but the indexing operator <code>[]</code> does have the advantage that it can handle column names with reserved characters in them (e.g. columns names separated by blank spaces such as <code>Country Name</code>, since <code>data.Country Name</code> would not work!)\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Exercise\n",
    "<hr>\n",
    "\n",
    "Can you select element 4 of the same `Series`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### 4.2 [Indexing using pandas syntaxis](#III.II)\n",
    "<a id=\"III.II\"></a>\n",
    "<br/>\n",
    "\n",
    "Pandas has its own accessor operatos, `loc[ ]` and `iloc[ ]`. For more advanced operations, these are the ones you are supposed to be using.\n",
    "\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Index-based selection `iloc[ ]`\n",
    "\n",
    "<br/>\n",
    "\n",
    "In pandas we have two paradigms for indexing. The first one we will review is the **index-based selection**. This simply means that we will be selecting data based on its numerical position in the `DataFrame` (as we did when we review `list`s). For this first paradigm we use the method `iloc[]`. \n",
    "\n",
    "To select the **first row** of our data `DataFrame`, we can do the follwoing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is returning a `Series` where the values of it are the first row of data in our `DataFrame`. And the `index` of them are the `column_names` of our `DataFrame`.\n",
    "\n",
    "Both `loc[]` and `iloc[]` are **row-first, column-second**. Please, note that this is the opposite of what we do in naitive Python, which is column-first, row-second.\n",
    "\n",
    "This means that using pandas accessors it is marginally easier to retrieve rows, and marginally harder to retrieve columns.\n",
    "\n",
    "To get a column by using `iloc[]` method we will need to specify two arguments `iloc[rows,columns]`.For example, lets select the column `Continent` using its numerical position on the data `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is returning a `Series` where the values of it are the **first column** of data in our `DataFrame`. And the `index` of them are the `row_labels` of our `DataFrame`.\n",
    "\n",
    "The operator `:` also comes from naitive Python and it **means \"everything\"**. When combinded with other selectors, it can be used to indicate a range of values. For example, to select the `Continent` column but just the first three rows we can do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to select just the from the same column rows 10 to 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b>  I introduced a new method of pandas <code>DataFrame</code>s that is  <code>reset_index()</code>. This new method allows us to reset the <code>row_labels</code>, recall that when we imported the data we asked the <code>index</code> to be equal to the <code>Country Code</code>. For presentation purposes in the code above I present the <code>row_labels</code> as 0, 1, 2, 3, ...  so it is clearer what the <code>iloc[10:14,1]</code> method is doing (also pay attention that now <code>Continent</code> column is the fifth one since the first position was occupied by <code>Country Code</code>).\n",
    "</div>\n",
    "\n",
    "\n",
    "It is also possible to pass a list to `iloc[]` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note to this **index-based selection** it is worth knowning that it is also posible to use negative numbers. This makes the `iloc[]` method to start counting forwards from the *end* of the values. So for example here are the last five elements of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### 4.2.2 Label-based selection `loc[ ]`\n",
    "\n",
    "<br/>\n",
    "\n",
    "The second paradigm in pandas accesses to `DataFrame`s is the one followed by the `loc[ ]` method. In this paradigm, it's the data **index value**, not its position, which matters.\n",
    "\n",
    "For example, as befor lets address the first observation fo the column `Continent`.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### 3.2.3 Differences between `loc[ ]`and `iloc[ ]`\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "The two methods use different indexing schemes.\n",
    "\n",
    "`iloc[ ]` uses a scheme where the **first** element of the range is **included** and the **last** one is **excluded**. So, for example `.iloc[0:10]` will select entries 0,...,9. In contrast, `.loc[0:10]` indexes inclusively, so will output entries 0,...,10.\n",
    "\n",
    "This difference is due to the fact that `loc[ ]` is meant to work indexing strings. So for example this characteristic is very convenient when we need to index a dataframe (let it be called `df`) that contains index values such as fruits: `Apples,...,Potatoes,...,` and we want to select \"all the alphabetical fruit choices between Apples and Potatoes\". Then `df.loc['Apples':'Potatoes']` has a much more intuitive use than something like `df.loc['Apples':'Potatoet']` (t comes after s in the alphabet).\n",
    "\n",
    "Otherwise, the semantics and use of `loc[ ]` are the same as those for `iloc[ ]`\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 [Manipulating the index](#III.III)\n",
    "<a id=\"III.III\"></a>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Label-based selection dervies its power from the lables in the index. So, we can manipulate the index in any way we see fit by using the method `set_index()`.\n",
    "\n",
    "For example, lets reset the index of our data. Remember that when first imported it we set the column `Time` as the new index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### 4.4 [Conditional Selection](#III.VI)\n",
    "<a id=\"III.VI\"></a>\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "This type of selection works well for **asking interesting questions** to our data. In particular, it helps to *ask* questions based on conditions.\n",
    "\n",
    "For example, suppose that for some reason we are interested in all european countries that have an average GDP of the period higher than 1,000,000,000,000 (or $10^{12}$).\n",
    "\n",
    "First, we need to check if each observation has the column `Continent` equal to **Europe**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation produced a `Series`of `True/False` booleans based on the continent of each record. This result can be used inside the `.loc[]` method to select the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `DataFrame` has $\\sim 37 k$ rows. The original had $\\sim 59 k$. Means that around $66\\%$ of the observations has the `Continent` column equal to **`Europe`**.\n",
    "\n",
    "We also wanted to know which ones have an average GDP higher than 1,000,000,000,000. For this purpose, I will present a new method: `Series.mean()`. This method and others that allow us to generate summary statistics will be reviewed further in detail.\n",
    "\n",
    "To implement the second condition we need to:\n",
    "\n",
    "1. extract the `Series` with the values of the Gross Domestic Product (it is called `GDP (constant 2010 US$)`\n",
    "2. compare the `value` in the `Series` with the desired one ($10^{12}$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the ampersand (`&`) symbol to bring the two questions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to select any observations in which the continent is equal to Europe *or* the GDP is above the threshold mentioned aboved. For this we use the pipe (`|`) symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### 3.3.1 Pandas built-in conditional selectors `.isin([])` and `.isnull()`\n",
    "\n",
    "<br/>\n",
    "\n",
    "Pandas comes with a few built-in conditional selectors. In this section we presented two of them.\n",
    "\n",
    "`.isin([])` helps to select data whose value \"is in\" a **list of values**. For example, we can use it to select observations from the [G7 countries](https://www.investopedia.com/terms/g/g5.asp). \n",
    "\n",
    "Such countries are **United States**, the **United Kingdom**, **Canada**, **Germany**, **Japan**, **Italy**, **France**, and, until recently, Russia. In 2014, Russia was suspended indefinitely from the group after annexing Crimea, an autonomous republic of Ukraine. As a result, the G8 is now often referred to as the **G7**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.isnull()` (and its counterpart `.notnull()`) let you highlight values which are (or are not) empty (`NaN`). \n",
    "\n",
    "For example, lets filter out all rows that contain missing values for `Series Name`. Here is how we do it by using this operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset contains $\\sim 59k$ entries. When filtered out all missing values we stay with $\\sim 42k$ registers.\n",
    "\n",
    "For concluding this section, let me stress out that I used operator `.loc[:,'Series Name']` that returns exactly the same `Series` that the one you can get by using the native Python operator `data['Series Name']`. \n",
    "\n",
    "In the following lines of code I will be changing indistinctly between those two ways of selection. Just to emphasize that they produce exactly the same output (when outputing a `Series`). Note that `.loc[]` additionally let us to perform the type of **conditional selection** that we reviewed during this section.\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 [Assigning data](#III.V)\n",
    "<a id=\"III.V\"></a>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Assigning data to a `Series` of column of a `DataFrame` is easy. It is possible to assign either a constant value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or iterable values, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For concluding this section, please note that the code above is using two native Python function `range()` and `len()`. The first one outputs an object that produces a sequence of integers from start (inclusive)\n",
    "to stop (exclusive) by step. The second one simply returns the number of items in the object (recall that `DataFrame` is always rows first columns second). Lastly, you can always review what a Python method does by asking after its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range?\n",
    "#len?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. [Summary Functions and Maps](#VI)\n",
    "<a id=\"VI\"></a>\n",
    "<br/>\n",
    "\n",
    "As data does not always come out of memory in the format we want it. Sometimes we need to do some more work ourselves to reformat it for the task at hand. In this section, we will be covering different opeartions that we can apply to our data to get the input \"just right\" for our models, presentations and so on. First lets see a group of functions that are usually and informally called *summary functions*.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 [Summary functions](#VI.I)\n",
    "<a id=\"VI.I\"></a>\n",
    "<br/>\n",
    "\n",
    "Pandas provides many simple *summary functions* which helps when you need to restructure your data in some useful way. For example, consider the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method generates a high-level summary of the attributes of the given column. It is **type-aware**, meaning that its output varies based on the data type of the column. The output above only makes sense for numerical data. In the case of string we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to get a particular simple summary about a column in a `DataFrame` or a `Series`, most of the time there is a helpful pandas function that can make it happen.\n",
    "\n",
    "For example, to see the mean of the Portuguese *population*in the available period, we can use the method `.mean()`.\n",
    "\n",
    "To do so, we need to:\n",
    "\n",
    "1. select the rows where the `Series Name` is equal to `Population, total`\n",
    "2. select the rows where the `Country Name` is equal to `Portugal`\n",
    "3. extract `value` series\n",
    "4. apply the method mentioned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other interesanting **method** of pandas `DataFrames` is `info()`. `info()` constructs a table that allow us to get the following information about the columns:\n",
    "+ name\n",
    "+ how many rows are non-null\n",
    "+ type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will return on talking about null values and columns types in a while.\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### 5.1.1 Finding unique values\n",
    "\n",
    "<br/>\n",
    "\n",
    "To see a list of unique values we can use the `.unique()` function.\n",
    "\n",
    "For example let's see the list of countries that this extraction has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### 5.1.2 Counting unique values\n",
    "\n",
    "<br/>\n",
    "\n",
    "To see a list of values and how often they occur in a data set we can use `value_counts()` method. For example, let's see how many observations per year we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### 5.2 [Maps](#VI.II)\n",
    "<a id=\"VI.II\"></a>\n",
    "<br/>\n",
    "A **map** is a term borrowed from mathematics. It can be seen as a function that takes one set of values and \"maps\" them to another set of values. In data science we often need to create new representations from existing data, or transform data from the format we recieve it (for example from our operations team) to the format we want it to be later for models or business presentations. Maps are what handle this type of work.\n",
    "\n",
    "There are two mapping methods that you will often use.\n",
    "\n",
    "`map()` is slightly the simpler one. For example, suppose that we need to remean the GDP to 0. We can do it in the following way.\n",
    "\n",
    "Again to do so, we need to:\n",
    "\n",
    "1. select the rows where the `Series Name` is equal to `Population, total`\n",
    "2. select the rows where the `Country Name` is equal to `Portugal`\n",
    "3. extract `value` series\n",
    "4. apply the method mentioned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function you pass to `map()` expects a single value from the `Series`, and return a transformed version of that value (in our example that same value minus the column mean). `map()` returns a new `Series` where all the values have been transformed by your function.\n",
    "<br/>\n",
    "<hr>\n",
    "\n",
    "#### 5.2.1 Quick review about Python's lambda expression\n",
    "\n",
    "<hr>\n",
    "<br/>\n",
    "Lambda expressions allow us to create \"anonymous\" functions. This basically means we can quickly make ad-hoc functions without needing to properly define a function using def. \n",
    "\n",
    "Function objects returned by running lambda expressions work exactly the same as those created and assigned by `def`s. There is a key difference that makes lambda useful in specialized roles. Pandas library works very well with lambda expressions.\n",
    "\n",
    "**lambda's body is a single expression, not a block of statements.**\n",
    "\n",
    "+ The lambda's body is similar to what we would put in a `def` body return statement. We simply type the result as an expression instead of explicitly returning it. Because it is limited to an expression, a lambda is less general that a def. We can only squeeze design, to limit program nesting. lambda is designed for coding simple functions, and def to handle the larger tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Lets generate a simple function and compare it with its equivalent lambda expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the from that a lambda expression trying to replicate the function above would take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output is a function (in our data set this function is passed to `map()` method so it can be applied to each of the elements of the column). So we need to assing it to a variable to then use it in the desired way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_square(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets generate a lambda function that will allow us to have a quick overview of any dataset that we may encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr>\n",
    "<hr>\n",
    "<br/>\n",
    "\n",
    "`apply()` is the equivalent mehtod if we want to transform a whole `DataFrame` by calling a custom method on each row. For example, lets define the mean_gdp function and then use it to transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we called the `port_gdp_data.apply()` with `axis='index'`, then instead of passing a function to transform each row, we would need to give a function to transform each **column** .\n",
    "\n",
    "Note that both `map()`and `apply()`methods return a new transformed `Series`and `DataFrame`respectively. They do not modify the original dataset they uses as input. If we look at the original `port_gdp_data` we can see that it still has its original `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### 5.2.2 **Built in mapping operators in Pandas**\n",
    "\n",
    "<br/>\n",
    "For the most common operators we need not to use explicitly the `map()` method. For example, here is a faster way of remeaning our `value` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the code above we are performing an operation between a lot of vales on the left-hand side (actually everyting in the `Series`) and a single value on the right-hand side (the mean value $230,698,913,940.09$). Pandas looks at this expression and figures out that we must mean to substract that value from every value in the column.\n",
    "\n",
    "Pandas will also understand what to do if we perform these operations between `Series` of equal length. For example, an easy way to combine the continent and country name of our data is by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above I introduced the method `.astype()` to transform `Time` from an integer column to a string column. \n",
    "\n",
    "This needs to be done because Pandas can only concatenate a string with other string or equivalently can only add a number with other number. Both operations are performed with the same `+` operator.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b>  Performing this operators rather than using <code>map()</code> or <code>apply()</code> methods is faster because they use speed ups built into Pandas. Actualy, all of the standard Python operators (greater, lesser or equal than) work in this manner.\n",
    "</div>\n",
    "\n",
    "However, they are not as flexible as `map()` or `apply()`, which can do more advanced things, like applying conditional logic, which cannot be done with addition and substraction alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 6. [Grouping and Sorting](#V)\n",
    "<a id=\"V\"></a>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 [Grouping](#V.I)\n",
    "<a id=\"V.I\"></a>\n",
    "<br/>\n",
    "\n",
    "Most data operations are done on groups defined by variables for creating them pandas `DataFrame`s have the method `groupby()`. The way it generally works is as follows:\n",
    "\n",
    "1. Define the *groups* based on a set of columns in your data\n",
    "2. Summarize the information of other columns at the grouped level\n",
    "\n",
    "For the second step the method you will need to use is `agg()`, this method will allow you to summarize a set of columns each of them with a specific summary function. The structure of the command is presented in the image below:\n",
    "<br/>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"820\" height=\"500\" src=\"https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2019/10/pandas-python-group-by-named-aggregation-update.jpg\">\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Now, let's make a simple table with summary statistcis of our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are aggregation functions already predifined by pandas below you can see a **non-exhaustive** list of them:\n",
    "\n",
    "\n",
    "| Function | Description  |\n",
    "| :-----: | :-----: |\n",
    "| count | Number of non-null observations |\n",
    "| sum | Sum of values |\n",
    "| mean | Mean of values |\n",
    "| mad | Mean absolute deviation |\n",
    "| median | Arithmetic median of values |\n",
    "| min | Minimum |\n",
    "| max | Maximum |\n",
    "| mode | Mode |\n",
    "| abs | Absolute Value |\n",
    "| prod | Product of values |\n",
    "| var | Unbiased variance |\n",
    "| sem | Unbiased standard error of the mean |\n",
    "| skew | Unbiased skewness (3rd moment) |\n",
    "| kurt | Unbiased kurtosis (4th moment) |\n",
    "| quantile | Sample quantile (value at %) |\n",
    "| cumsum | Cumulative sum |\n",
    "| cumprod | Sample quantile (value at %) |\n",
    "| quantile | Cumulative product |\n",
    "| cummax | Cumulative maximum |\n",
    "| cummin | Cumulative minimum |\n",
    "\n",
    "Note that it is also possible to create your own aggregation functions. However, the need for custom functions is minimal unless you have very specific requirements. The full range of basic statistics that are quickly calculable and built into the base Pandas package can be found [here](https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html)\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 [Sorting](#V.II)\n",
    "<a id=\"V.II\"></a>\n",
    "<br/>\n",
    "\n",
    "Now, imagine you need to sort in a descending way the results shown above. By using the `DataFrame` method `sort_values()` and selecting the parameter `ascending=False` this process can be easily performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 7. [Data Types and Missing Values](#IV)\n",
    "<a id=\"IV\"></a>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 [Data Types](#IV.I)\n",
    "<a id=\"IV.I\"></a>\n",
    "<br/>\n",
    "\n",
    "At the end of the prior section we introduced an operator to modify the type a specific column. The correct name of the type of a column in a `DataFrame` (or `Series`) is **dtype**.\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Specific Column\n",
    "<br/>\n",
    "\n",
    "So, you can use the `dtype` attribute of a `Series` to grab the type of a specific column. For instance, we can get the `dtype` of the `Time` column in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "#### 7.1.2 Every column in the dataset\n",
    "<br/>\n",
    "Alternatively, the `dtypes` attribute of a `DataFrame` returns the `dtype`of *every* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types tell us something about how pandas is storing the data internally. For example, `float64` means that it is using a 64-bit floating point number, whereas `int64` means a similarly sized integer instead.\n",
    "\n",
    "One peculiarity to keep in mind is that columns consisting entirely of strings do not get their own type, instead they are given the `object` type.\n",
    "\n",
    "As we presented before, function `.astype()` makes possible to convert a column of one type into another wherever such a convertion makes sense. \n",
    "\n",
    "A `DataFrame` or `Series` index has its own `dtype` too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note to this section, let's remark that Pandas also supports more exoctic data types. For example, categorical data and timeseries data are also allowed.\n",
    "\n",
    "### 7.2 [Missing data](#IV.II)\n",
    "<a id=\"IV.II\"></a>\n",
    "<br/>\n",
    "Entries with missing values are given the value NaN, short for \"Not a Number\". For technical reasons these `NaN` values are always of the `float64` dtype.\n",
    "<br/>\n",
    "\n",
    "#### 7.2.1 Selecting missing rows data\n",
    "<br/>\n",
    "Pandas provides some methods specific to missing data. To select `NaN` entries you can use `pd.isnull()` (or its companion `pd.notnull()`). For example, lets output all the rows of our dataset that has `value` column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing missing values is a common operation. Pandas provides a handy method for this problem: `fillna()`. `fillna()` provides a few different strategies for mitigating such data. For example, we can simply replace each `NaN`with an `\"Unknown\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we may have a none-null value that we would like to replace. For example, lets `replace()` the string value of `Continent` column from 'World' to 0, 'America' to 1, 'Europe' to 2, 'Middle East' to 3, 'Africa' to 4, 'Asia' to 5 and 'Oceania' to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 8. [Renaming and Combining](#IIV)\n",
    "<a id=\"IIV\"></a>\n",
    "<br/>\n",
    "Data comes often to us with column names, index names or other naming conventions that we are not satisfied with. In that case, we will learn how to use pandas functions to change the names of the entries we need to.\n",
    "\n",
    "We will also explore three methods to combine data from multiple `DataFame` and/or `Series`.\n",
    "<br/>\n",
    "\n",
    "### 8.1 [Renaming](#IIV.I)\n",
    "<a id=\"IIV.I\"></a>\n",
    "<br/>\n",
    "The first function to introduce here is `rename()`, which allows you to change index names and/or column names. For example, to change the `value` column in our dataset to `Series Value`, we would do:\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rename()` supports a variety of input formats, but usually a Python dictionary is the most convenient. For **modifying a set of columns** we leverage two core Python operators: `dict()` and `zip()`. The first one simply produces a dictionary (is equivalent to `{ }`), the second one is the `zip()` function that *paste* one on one each of the elements of two list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rename()` lets you rename *index* or *column* values by specifying `index` or `column` keyword parameter, respectively. Here is an example using it to rename some elements of the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common to rename columns, but rename index values is very rarely. For doing so it is usually more convenient to use the `set_index()` method.\n",
    "<br/>\n",
    "\n",
    "#### 8.1.1 Renaming axis\n",
    "<br/>\n",
    "Both rows and columns have their own name attribute. So, additionaly to renaming values of each axis  `rename_axis()` method may be used to change these name attribute (naming the hole rows or the hole columns). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### 8.2 [Combining](#IIV.II)\n",
    "<a id=\"IIV.II\"></a>\n",
    "<br/>\n",
    "\n",
    "When working on a dataset, we will sometimes need to combine different `DataFrames` and/or `Series` in *non-trivial* ways. Pandas has three core methods for doing this. Ranked on increasing complexity, these methods are `concat()`, `join()`, and `merge()`.\n",
    "\n",
    "As its name says, `concat()` will concatenate two `DataFrames` together along an axis. This method is very useful when working with data in different `DataFrames` or `Series` objects but having the same fields (columns) . \n",
    "\n",
    "For example imagine that we need to create a report with a subset of the available goverment data from Portugal of the period. The indicators that we need to present are the following:\n",
    "\n",
    "+ GDP (constant 2010 US\\$)\n",
    "+ Expense (\\% of GDP)\n",
    "+ Government expenditure on education, total (\\% of GDP)\n",
    "+ Domestic general government health expenditure (\\% of GDP)\n",
    "\n",
    "For this we will need to create a subset of the original dataframe with all the series available for Portugal. With it we will need to:\n",
    "\n",
    "+ select all the rows where `Series Name` equals each of the desired indicators\n",
    "+ rename `Series Name` to fit the indicator description\n",
    "+ use the method mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all let's select only entries reported by Portugal\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then let's set the conditions to select the data\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then select and rename each series\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally let's concatenate all together\n",
    "#and put back together the column names since concate operator loss them\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The middlemost complex combiner is `join()`. It works for combining different `DataFrame` objects which have a column (or index) in common. For a clear explanation of its usage and its parameters, let's generate two new `DataFrames` each of them with two columns named: `Key` and `Values`.  \n",
    "\n",
    "The first one name it `right` and fill each column with the following values:\n",
    "\n",
    "+ `Key`: 2000,2001,2002,2003\n",
    "+ `Values`:12,13,12,13 \n",
    "\n",
    "The second one name it `left` and fill each column with the following values:\n",
    "\n",
    "+ `Key`: 2000,2001,2003\n",
    "+ `Values`:15,16,17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets *join* this to datasets using operator `.join()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `on` parameter allows us to to join the datasets using the `Key` column, however `DataFrame.join()` **always uses the second one index** so that's why in the code above we use the method `set_index('Key')` on the second `DataFrame` (in the example is called `left`).\n",
    "\n",
    "Back to our example database, imagine that now we have a report with another set of indicators:\n",
    "\n",
    "+ Final consumption expenditure (\\% of GDP): (formerly total consumption) is the sum of household final consumption expenditure (private consumption) and general government final consumption expenditure (general government consumption)\n",
    "+ Exports of goods and services (\\% of GDP)\n",
    "\n",
    "And we want join them together in a single dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's set the conditions to select the data\n",
    "#type your code here'\n",
    "\n",
    "#Then select and rename each series\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the new dataset has exactly the same amount of rows (or observations) that our first dataset. `join()` method has set by default the value of the parameter `how` to \"left\". So, the code above is *pasting*  two additional columns using the values of `Time` (set as index in both datasets) as keys for this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last combining operator is `merge()`, it is said to be more complicated simply because it has more parameters to specify. This allows more flexibility when dealing with difficult *types* of combinations. \n",
    "\n",
    "Differently from `join()` it supports any column or index as *key* for the combination to perform. It also has set as default `how` value \"inner\" (so by default it would just return rows in which key values belong to the intersection of both dataset). \n",
    "\n",
    "Let's repeat what we have done so far using this operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is important to remark that almost everything that can be done with `merge()` can also be done with `join()`. However, `merge()` gives you more control on how pandas is  internally procesing your data. In particular, the **optional parameter** `validate` helps you check how pandas is processing internally the key values. In our example, I specified \"1:1\" that means \"one to one\" because it makes pandas to check if merge keys are unique on the both dataset.\n",
    "\n",
    "Remember that you can always *ask Python* for help when you feel confused about the parameters or its usage of both `merge()` and `join()` (or any other) operator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
